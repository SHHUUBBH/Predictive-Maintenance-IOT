{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Predictive Maintenance for Industrial IoT\n",
        "## Step 7: Documentation & Dashboard\n",
        "\n",
        "This notebook covers:\n",
        "1. Creating an interactive dashboard with Streamlit\n",
        "2. Visualizing model predictions and results\n",
        "3. Building a monitoring interface for real-time data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display all columns in pandas dataframes\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 7.1: Creating a Streamlit Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streamlit app code saved to ../src/dashboard/app.py\n",
            "Dashboard requirements saved to ../src/dashboard/requirements.txt\n",
            "Dashboard README saved to ../src/dashboard/README.md\n",
            "\n",
            "Dashboard files created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create a Streamlit dashboard\n",
        "# Note: We can't run Streamlit directly in a Jupyter notebook\n",
        "# Instead, we'll create a Streamlit app file that can be run separately\n",
        "\n",
        "streamlit_app_code = \"\"\"\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "import os\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(\n",
        "    page_title=\"Predictive Maintenance Dashboard\",\n",
        "    page_icon=\"ðŸ”§\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "\n",
        "# Define functions for loading data and models\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    # In a real application, you would load your actual data\n",
        "    # For demonstration, we'll create synthetic data\n",
        "    \n",
        "    # Create a date range for the past 30 days\n",
        "    dates = pd.date_range(end=datetime.now(), periods=30*24, freq='H')\n",
        "    \n",
        "    # Create sensor readings with some random noise\n",
        "    data = pd.DataFrame({\n",
        "        'timestamp': dates,\n",
        "        'sensor_1': np.sin(np.linspace(0, 15*np.pi, len(dates))) + np.random.normal(0, 0.1, len(dates)),\n",
        "        'sensor_2': np.cos(np.linspace(0, 15*np.pi, len(dates))) + np.random.normal(0, 0.1, len(dates)),\n",
        "        'sensor_3': np.random.normal(0, 0.2, len(dates)).cumsum(),\n",
        "        'sensor_4': np.random.normal(1, 0.1, len(dates)),\n",
        "        'sensor_5': np.random.normal(0, 0.3, len(dates)).cumsum() + np.sin(np.linspace(0, 5*np.pi, len(dates)))\n",
        "    })\n",
        "    \n",
        "    # Add some anomalies\n",
        "    anomaly_indices = [100, 250, 400, 550, 650]\n",
        "    for idx in anomaly_indices:\n",
        "        data.loc[idx:idx+10, 'sensor_1'] += 2.0\n",
        "        data.loc[idx:idx+10, 'sensor_2'] -= 1.5\n",
        "        data.loc[idx:idx+10, 'sensor_4'] *= 1.5\n",
        "    \n",
        "    # Add a failure flag column (1 for failure, 0 for normal)\n",
        "    data['failure'] = 0\n",
        "    for idx in anomaly_indices:\n",
        "        data.loc[idx+5:idx+15, 'failure'] = 1\n",
        "    \n",
        "    return data\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    # In a real application, you would load your trained model\n",
        "    # For demonstration, we'll create a simple function that returns random predictions\n",
        "    def dummy_model(data):\n",
        "        # Calculate anomaly scores based on sensor readings\n",
        "        scores = (\n",
        "            np.abs(data['sensor_1']) + \n",
        "            np.abs(data['sensor_2']) + \n",
        "            np.abs(data['sensor_3']) + \n",
        "            np.abs(data['sensor_4']) + \n",
        "            np.abs(data['sensor_5'])\n",
        "        ) / 5\n",
        "        \n",
        "        # Normalize scores to [0, 1]\n",
        "        scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "        \n",
        "        # Return predictions and probabilities\n",
        "        predictions = (scores > 0.7).astype(int)\n",
        "        probabilities = scores\n",
        "        \n",
        "        return predictions, probabilities\n",
        "    \n",
        "    return dummy_model\n",
        "\n",
        "# Load data and model\n",
        "data = load_data()\n",
        "model = load_model()\n",
        "\n",
        "# Make predictions\n",
        "predictions, probabilities = model(data)\n",
        "data['prediction'] = predictions\n",
        "data['probability'] = probabilities\n",
        "\n",
        "# Calculate metrics\n",
        "true_positives = ((data['prediction'] == 1) & (data['failure'] == 1)).sum()\n",
        "false_positives = ((data['prediction'] == 1) & (data['failure'] == 0)).sum()\n",
        "true_negatives = ((data['prediction'] == 0) & (data['failure'] == 0)).sum()\n",
        "false_negatives = ((data['prediction'] == 0) & (data['failure'] == 1)).sum()\n",
        "\n",
        "accuracy = (true_positives + true_negatives) / len(data)\n",
        "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "# Create the Streamlit dashboard\n",
        "st.title(\"Predictive Maintenance Dashboard\")\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.header(\"Settings\")\n",
        "st.sidebar.subheader(\"Filter Data\")\n",
        "\n",
        "# Date range filter\n",
        "date_range = st.sidebar.date_input(\n",
        "    \"Select Date Range\",\n",
        "    [data['timestamp'].min().date(), data['timestamp'].max().date()]\n",
        ")\n",
        "\n",
        "# Convert date_range to datetime for filtering\n",
        "start_date = pd.Timestamp(date_range[0])\n",
        "end_date = pd.Timestamp(date_range[1]) + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n",
        "\n",
        "# Filter data based on date range\n",
        "filtered_data = data[(data['timestamp'] >= start_date) & (data['timestamp'] <= end_date)]\n",
        "\n",
        "# Sensor selection\n",
        "st.sidebar.subheader(\"Select Sensors\")\n",
        "selected_sensors = st.sidebar.multiselect(\n",
        "    \"Choose sensors to display\",\n",
        "    ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5'],\n",
        "    default=['sensor_1', 'sensor_2']\n",
        ")\n",
        "\n",
        "# Threshold for anomaly detection\n",
        "threshold = st.sidebar.slider(\"Anomaly Detection Threshold\", 0.0, 1.0, 0.7, 0.01)\n",
        "\n",
        "# Dashboard content\n",
        "# Row 1: Key metrics\n",
        "st.header(\"Key Metrics\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "\n",
        "with col1:\n",
        "    st.metric(\"Accuracy\", f\"{accuracy:.2%}\")\n",
        "    \n",
        "with col2:\n",
        "    st.metric(\"Precision\", f\"{precision:.2%}\")\n",
        "    \n",
        "with col3:\n",
        "    st.metric(\"Recall\", f\"{recall:.2%}\")\n",
        "    \n",
        "with col4:\n",
        "    st.metric(\"F1 Score\", f\"{f1:.2%}\")\n",
        "\n",
        "# Row 2: Sensor readings and anomaly detection\n",
        "st.header(\"Sensor Readings and Anomaly Detection\")\n",
        "\n",
        "# Create a plotly figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add sensor readings\n",
        "for sensor in selected_sensors:\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=filtered_data['timestamp'],\n",
        "        y=filtered_data[sensor],\n",
        "        mode='lines',\n",
        "        name=sensor\n",
        "    ))\n",
        "\n",
        "# Add anomaly markers\n",
        "anomalies = filtered_data[filtered_data['probability'] > threshold]\n",
        "if not anomalies.empty:\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=anomalies['timestamp'],\n",
        "        y=[0] * len(anomalies),  # Place markers at the bottom\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            symbol='x',\n",
        "            color='red',\n",
        "            size=10\n",
        "        ),\n",
        "        name='Detected Anomalies'\n",
        "    ))\n",
        "\n",
        "# Add true failures\n",
        "failures = filtered_data[filtered_data['failure'] == 1]\n",
        "if not failures.empty:\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=failures['timestamp'],\n",
        "        y=[0] * len(failures),  # Place markers at the bottom\n",
        "        mode='markers',\n",
        "        marker=dict(\n",
        "            symbol='circle',\n",
        "            color='black',\n",
        "            size=10\n",
        "        ),\n",
        "        name='True Failures'\n",
        "    ))\n",
        "\n",
        "# Update layout\n",
        "fig.update_layout(\n",
        "    title='Sensor Readings Over Time',\n",
        "    xaxis_title='Time',\n",
        "    yaxis_title='Value',\n",
        "    height=500,\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "st.plotly_chart(fig, use_container_width=True)\n",
        "\n",
        "# Row 3: Anomaly probability\n",
        "st.header(\"Anomaly Probability\")\n",
        "\n",
        "# Create a plotly figure for anomaly probability\n",
        "fig_prob = go.Figure()\n",
        "\n",
        "# Add probability line\n",
        "fig_prob.add_trace(go.Scatter(\n",
        "    x=filtered_data['timestamp'],\n",
        "    y=filtered_data['probability'],\n",
        "    mode='lines',\n",
        "    name='Anomaly Probability'\n",
        "))\n",
        "\n",
        "# Add threshold line\n",
        "fig_prob.add_trace(go.Scatter(\n",
        "    x=filtered_data['timestamp'],\n",
        "    y=[threshold] * len(filtered_data),\n",
        "    mode='lines',\n",
        "    line=dict(color='red', dash='dash'),\n",
        "    name=f'Threshold ({threshold})'\n",
        "))\n",
        "\n",
        "# Update layout\n",
        "fig_prob.update_layout(\n",
        "    title='Anomaly Probability Over Time',\n",
        "    xaxis_title='Time',\n",
        "    yaxis_title='Probability',\n",
        "    height=400,\n",
        "    legend=dict(\n",
        "        orientation=\"h\",\n",
        "        yanchor=\"bottom\",\n",
        "        y=1.02,\n",
        "        xanchor=\"right\",\n",
        "        x=1\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the plot\n",
        "st.plotly_chart(fig_prob, use_container_width=True)\n",
        "\n",
        "# Row 4: Recent anomalies table\n",
        "st.header(\"Recent Anomalies\")\n",
        "\n",
        "# Get recent anomalies\n",
        "recent_anomalies = filtered_data[filtered_data['probability'] > threshold].tail(10)\n",
        "if not recent_anomalies.empty:\n",
        "    # Format the table\n",
        "    table_data = recent_anomalies[['timestamp', 'probability'] + selected_sensors].copy()\n",
        "    table_data['timestamp'] = table_data['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    table_data['probability'] = table_data['probability'].map('{:.2%}'.format)\n",
        "    \n",
        "    # Display the table\n",
        "    st.dataframe(table_data, use_container_width=True)\n",
        "else:\n",
        "    st.info(\"No anomalies detected in the selected time range.\")\n",
        "\n",
        "# Row 5: Confusion matrix\n",
        "st.header(\"Model Performance\")\n",
        "\n",
        "col1, col2 = st.columns(2)\n",
        "\n",
        "with col1:\n",
        "    # Create confusion matrix\n",
        "    cm = np.array([[true_negatives, false_positives], [false_negatives, true_positives]])\n",
        "    \n",
        "    # Create a plotly figure for confusion matrix\n",
        "    fig_cm = px.imshow(\n",
        "        cm,\n",
        "        labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
        "        x=['Normal', 'Failure'],\n",
        "        y=['Normal', 'Failure'],\n",
        "        text_auto=True,\n",
        "        color_continuous_scale='Blues'\n",
        "    )\n",
        "    \n",
        "    # Update layout\n",
        "    fig_cm.update_layout(\n",
        "        title='Confusion Matrix',\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    # Display the plot\n",
        "    st.plotly_chart(fig_cm, use_container_width=True)\n",
        "\n",
        "with col2:\n",
        "    # Create ROC curve (simplified for demonstration)\n",
        "    fpr = [0, false_positives / (false_positives + true_negatives), 1]\n",
        "    tpr = [0, true_positives / (true_positives + false_negatives), 1]\n",
        "    \n",
        "    # Create a plotly figure for ROC curve\n",
        "    fig_roc = go.Figure()\n",
        "    \n",
        "    # Add ROC curve\n",
        "    fig_roc.add_trace(go.Scatter(\n",
        "        x=fpr,\n",
        "        y=tpr,\n",
        "        mode='lines+markers',\n",
        "        name='ROC Curve'\n",
        "    ))\n",
        "    \n",
        "    # Add diagonal line\n",
        "    fig_roc.add_trace(go.Scatter(\n",
        "        x=[0, 1],\n",
        "        y=[0, 1],\n",
        "        mode='lines',\n",
        "        line=dict(color='gray', dash='dash'),\n",
        "        name='Random'\n",
        "    ))\n",
        "    \n",
        "    # Update layout\n",
        "    fig_roc.update_layout(\n",
        "        title='ROC Curve',\n",
        "        xaxis_title='False Positive Rate',\n",
        "        yaxis_title='True Positive Rate',\n",
        "        height=400\n",
        "    )\n",
        "    \n",
        "    # Display the plot\n",
        "    st.plotly_chart(fig_roc, use_container_width=True)\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Predictive Maintenance Dashboard | Created with Streamlit\")\n",
        "\"\"\"\n",
        "\n",
        "# Save the Streamlit app code to a file\n",
        "streamlit_app_path = \"../src/dashboard/app.py\"\n",
        "os.makedirs(os.path.dirname(streamlit_app_path), exist_ok=True)\n",
        "with open(streamlit_app_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(streamlit_app_code)\n",
        "print(f\"Streamlit app code saved to {streamlit_app_path}\")\n",
        "\n",
        "# Create requirements.txt for the dashboard\n",
        "dashboard_requirements = \"\"\"\n",
        "streamlit==1.22.0\n",
        "pandas==2.0.1\n",
        "numpy==1.24.3\n",
        "matplotlib==3.7.1\n",
        "seaborn==0.12.2\n",
        "plotly==5.14.1\n",
        "scikit-learn==1.2.2\n",
        "requests==2.28.2\n",
        "\"\"\"\n",
        "\n",
        "dashboard_requirements_path = \"../src/dashboard/requirements.txt\"\n",
        "with open(dashboard_requirements_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(dashboard_requirements)\n",
        "print(f\"Dashboard requirements saved to {dashboard_requirements_path}\")\n",
        "\n",
        "# Create a README file for the dashboard\n",
        "dashboard_readme = \"\"\"\n",
        "# Predictive Maintenance Dashboard\n",
        "\n",
        "This dashboard provides a visual interface for monitoring equipment health and predicting failures.\n",
        "\n",
        "## Features\n",
        "\n",
        "- Real-time monitoring of sensor data\n",
        "- Anomaly detection and visualization\n",
        "- Performance metrics for the predictive model\n",
        "- Historical data analysis\n",
        "\n",
        "## Installation\n",
        "\n",
        "1. Install the required packages:\n",
        "   ```\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2. Run the dashboard:\n",
        "   ```\n",
        "   streamlit run app.py\n",
        "   ```\n",
        "\n",
        "3. Open your browser and navigate to http://localhost:8501\n",
        "\n",
        "## Usage\n",
        "\n",
        "- Use the sidebar to filter data by date range and select sensors to display\n",
        "- Adjust the anomaly detection threshold to control sensitivity\n",
        "- View real-time sensor readings and detected anomalies\n",
        "- Monitor model performance metrics\n",
        "\n",
        "## Customization\n",
        "\n",
        "To customize the dashboard for your specific use case:\n",
        "\n",
        "1. Modify the `load_data()` function to load your actual data\n",
        "2. Update the `load_model()` function to use your trained model\n",
        "3. Adjust the visualizations and metrics as needed\n",
        "\"\"\"\n",
        "\n",
        "dashboard_readme_path = \"../src/dashboard/README.md\"\n",
        "with open(dashboard_readme_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(dashboard_readme)\n",
        "print(f\"Dashboard README saved to {dashboard_readme_path}\")\n",
        "\n",
        "print(\"\\nDashboard files created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Step 7.2: Running the Dashboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instructions for running the dashboard\n",
        "print(\"To run the Streamlit dashboard, follow these steps:\")\n",
        "print(\"1. Open a terminal and navigate to the dashboard directory:\")\n",
        "print(\"   cd src/dashboard\")\n",
        "print(\"2. Install the required packages:\")\n",
        "print(\"   pip install -r requirements.txt\")\n",
        "print(\"3. Run the Streamlit app:\")\n",
        "print(\"   streamlit run app.py\")\n",
        "print(\"4. Open your browser and navigate to http://localhost:8501\")\n",
        "\n",
        "print(\"\\nThe dashboard provides the following features:\")\n",
        "print(\"- Real-time monitoring of sensor data\")\n",
        "print(\"- Anomaly detection and visualization\")\n",
        "print(\"- Performance metrics for the predictive model\")\n",
        "print(\"- Historical data analysis\")\n",
        "\n",
        "print(\"\\nYou can customize the dashboard by:\")\n",
        "print(\"1. Modifying the `load_data()` function to load your actual data\")\n",
        "print(\"2. Updating the `load_model()` function to use your trained model\")\n",
        "print(\"3. Adjusting the visualizations and metrics as needed\")\n",
        "\n",
        "# Create a screenshot of the dashboard (for demonstration purposes)\n",
        "dashboard_screenshot = \"\"\"\n",
        "+-----------------------------------------------------------------------+\n",
        "|                     Predictive Maintenance Dashboard                   |\n",
        "+---------------+---------------------------------------------------+\n",
        "| Settings      |  Key Metrics                                       |\n",
        "| ------------- |  +---------+ +---------+ +---------+ +---------+   |\n",
        "| Filter Data   |  | Accuracy| |Precision| | Recall  | |F1 Score |   |\n",
        "|               |  |  95.2%  | |  78.6%  | |  82.1%  | |  80.3%  |   |\n",
        "| Date Range    |  +---------+ +---------+ +---------+ +---------+   |\n",
        "| [Start] [End] |                                                     |\n",
        "|               |  Sensor Readings and Anomaly Detection              |\n",
        "| Select Sensors|  +---------------------------------------------------+\n",
        "| [x] Sensor 1  |  |                                                   |\n",
        "| [x] Sensor 2  |  |                 [Line Chart with Anomalies]       |\n",
        "| [ ] Sensor 3  |  |                                                   |\n",
        "| [ ] Sensor 4  |  |                                                   |\n",
        "| [ ] Sensor 5  |  +---------------------------------------------------+\n",
        "|               |                                                     |\n",
        "| Threshold     |  Anomaly Probability                                |\n",
        "| [====|===]    |  +---------------------------------------------------+\n",
        "|               |  |                                                   |\n",
        "|               |  |           [Probability Chart with Threshold]      |\n",
        "|               |  |                                                   |\n",
        "|               |  +---------------------------------------------------+\n",
        "|               |                                                     |\n",
        "|               |  Recent Anomalies                                   |\n",
        "|               |  +---------------------------------------------------+\n",
        "|               |  | Timestamp | Probability | Sensor 1 | Sensor 2 |   |\n",
        "|               |  | 2023-05-01| 85.2%       | 1.23     | -0.45    |   |\n",
        "|               |  | 2023-05-02| 92.1%       | 2.34     | -1.67    |   |\n",
        "|               |  +---------------------------------------------------+\n",
        "|               |                                                     |\n",
        "|               |  Model Performance                                  |\n",
        "|               |  +-------------------+ +-------------------+        |\n",
        "|               |  |                   | |                   |        |\n",
        "|               |  |  Confusion Matrix | |    ROC Curve      |        |\n",
        "|               |  |                   | |                   |        |\n",
        "|               |  +-------------------+ +-------------------+        |\n",
        "+---------------+---------------------------------------------------+\n",
        "|              Predictive Maintenance Dashboard | Created with Streamlit |\n",
        "+-----------------------------------------------------------------------+\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nDashboard Preview:\")\n",
        "print(dashboard_screenshot)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 7.3: Project Documentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive project documentation\n",
        "project_documentation = \"\"\"\n",
        "# Predictive Maintenance for Industrial IoT - Project Documentation\n",
        "\n",
        "## Project Overview\n",
        "\n",
        "This project implements a predictive maintenance system for industrial IoT devices using machine learning techniques. It analyzes sensor data to detect anomalies and predict potential failures before they occur.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "The project uses the APS Failure at Scania Trucks dataset, which contains sensor readings from the Air Pressure System (APS) of Scania trucks. The dataset consists of:\n",
        "\n",
        "- Training set: 60,000 examples (59,000 negative, 1,000 positive)\n",
        "- Test set: 16,000 examples\n",
        "- 171 attributes (anonymized sensor readings)\n",
        "- Binary classification task: positive class (APS component failures) vs. negative class (non-APS related failures)\n",
        "\n",
        "## Project Workflow\n",
        "\n",
        "### Step 1: Data Collection & Preprocessing\n",
        "- Dataset selection: APS Failure at Scania Trucks\n",
        "- Data loading and exploration\n",
        "- Handling missing values with median imputation\n",
        "- Feature scaling with StandardScaler\n",
        "- Feature importance analysis with Random Forest\n",
        "\n",
        "### Step 2: Exploratory Data Analysis (EDA)\n",
        "- Visualization of sensor readings\n",
        "- Analysis of correlations between features\n",
        "- Stationarity testing with ADF\n",
        "- Documentation of findings\n",
        "\n",
        "### Step 3: Anomaly Detection\n",
        "- Implementation of three anomaly detection algorithms:\n",
        "  - Isolation Forest\n",
        "  - One-Class SVM\n",
        "  - Autoencoder\n",
        "- Evaluation using precision, recall, and F1 score\n",
        "- Visualization of anomalies over time\n",
        "\n",
        "### Step 4: Predictive Modeling\n",
        "- ARIMA for time series prediction\n",
        "- LSTM for sequence modeling\n",
        "- Random Forest/XGBoost for classification\n",
        "- Train-test split and model evaluation\n",
        "\n",
        "### Step 5: Model Evaluation & Selection\n",
        "- Cross-validation for robust evaluation\n",
        "- Hyperparameter tuning for model optimization\n",
        "- Final model selection based on performance metrics\n",
        "- Saving the best model for deployment\n",
        "\n",
        "### Step 6: Cloud Deployment\n",
        "- Creating a Flask API for model deployment\n",
        "- Preparing the model for deployment\n",
        "- Testing the API locally\n",
        "- Guidelines for cloud deployment (AWS, Azure, GCP)\n",
        "\n",
        "### Step 7: Documentation & Dashboard\n",
        "- Creating an interactive dashboard with Streamlit\n",
        "- Visualizing model predictions and results\n",
        "- Building a monitoring interface for real-time data\n",
        "\n",
        "## Key Findings\n",
        "\n",
        "1. **Data Quality**:\n",
        "   - The dataset contains many missing values, which were handled using median imputation\n",
        "   - No duplicate samples were found\n",
        "   - All features are numerical (after conversion)\n",
        "\n",
        "2. **Feature Importance**:\n",
        "   - Top features were identified using Random Forest\n",
        "   - These features were used for modeling to reduce dimensionality\n",
        "\n",
        "3. **Anomaly Detection**:\n",
        "   - Isolation Forest achieved the best precision\n",
        "   - Autoencoder achieved the best recall\n",
        "   - One-Class SVM provided a good balance between precision and recall\n",
        "\n",
        "4. **Predictive Modeling**:\n",
        "   - ARIMA provided good short-term forecasts\n",
        "   - LSTM captured complex patterns in the time series data\n",
        "   - XGBoost achieved the best classification performance\n",
        "\n",
        "5. **Model Evaluation**:\n",
        "   - Cross-validation confirmed the robustness of the models\n",
        "   - Hyperparameter tuning improved model performance\n",
        "   - The best model was selected based on F1 score\n",
        "\n",
        "## Usage Instructions\n",
        "\n",
        "### Running the Notebooks\n",
        "\n",
        "1. Install the required packages:\n",
        "   ```\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "2. Run the Jupyter notebooks in sequence:\n",
        "   ```\n",
        "   jupyter notebook notebooks/01_data_processing_and_eda.ipynb\n",
        "   jupyter notebook notebooks/02_anomaly_detection.ipynb\n",
        "   jupyter notebook notebooks/03_predictive_modeling.ipynb\n",
        "   jupyter notebook notebooks/04_model_evaluation.ipynb\n",
        "   jupyter notebook notebooks/05_deployment.ipynb\n",
        "   jupyter notebook notebooks/06_dashboard.ipynb\n",
        "   ```\n",
        "\n",
        "### Running the API\n",
        "\n",
        "1. Navigate to the deployment directory:\n",
        "   ```\n",
        "   cd src/deployment\n",
        "   ```\n",
        "\n",
        "2. Install the required packages:\n",
        "   ```\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. Run the Flask app:\n",
        "   ```\n",
        "   python app.py\n",
        "   ```\n",
        "\n",
        "4. The API will be available at http://localhost:5000\n",
        "\n",
        "### Running the Dashboard\n",
        "\n",
        "1. Navigate to the dashboard directory:\n",
        "   ```\n",
        "   cd src/dashboard\n",
        "   ```\n",
        "\n",
        "2. Install the required packages:\n",
        "   ```\n",
        "   pip install -r requirements.txt\n",
        "   ```\n",
        "\n",
        "3. Run the Streamlit app:\n",
        "   ```\n",
        "   streamlit run app.py\n",
        "   ```\n",
        "\n",
        "4. The dashboard will be available at http://localhost:8501\n",
        "\n",
        "## Future Work\n",
        "\n",
        "1. **Data Collection**:\n",
        "   - Incorporate real-time sensor data\n",
        "   - Expand to additional sensor types\n",
        "\n",
        "2. **Model Improvement**:\n",
        "   - Explore deep learning architectures for time series forecasting\n",
        "   - Implement ensemble methods for improved performance\n",
        "   - Incorporate domain knowledge into feature engineering\n",
        "\n",
        "3. **Deployment**:\n",
        "   - Deploy the model to a cloud platform\n",
        "   - Implement CI/CD pipeline for model updates\n",
        "   - Add authentication and security features\n",
        "\n",
        "4. **Dashboard**:\n",
        "   - Add real-time monitoring capabilities\n",
        "   - Implement alert systems for detected anomalies\n",
        "   - Enhance visualizations for better interpretability\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This project demonstrates the effectiveness of machine learning techniques for predictive maintenance in industrial IoT applications. By detecting anomalies and predicting failures before they occur, the system can help reduce downtime, maintenance costs, and improve overall operational efficiency.\n",
        "\n",
        "The combination of time series forecasting, anomaly detection, and classification models provides a comprehensive approach to predictive maintenance. The interactive dashboard and API allow for easy integration into existing industrial systems.\n",
        "\"\"\"\n",
        "\n",
        "# Save the project documentation to a file\n",
        "documentation_path = \"../project_documentation.md\"\n",
        "with open(documentation_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(project_documentation)\n",
        "print(f\"Project documentation saved to {documentation_path}\")\n",
        "\n",
        "print(\"\\nProject Documentation Summary:\")\n",
        "print(\"1. Project Overview\")\n",
        "print(\"2. Dataset Description\")\n",
        "print(\"3. Project Workflow (Steps 1-7)\")\n",
        "print(\"4. Key Findings\")\n",
        "print(\"5. Usage Instructions\")\n",
        "print(\"6. Future Work\")\n",
        "print(\"7. Conclusion\")\n",
        "\n",
        "print(\"\\nThe documentation provides a comprehensive overview of the project, including:\")\n",
        "print(\"- The data preprocessing and exploration steps\")\n",
        "print(\"- The anomaly detection and predictive modeling techniques used\")\n",
        "print(\"- The model evaluation and selection process\")\n",
        "print(\"- Instructions for running the API and dashboard\")\n",
        "print(\"- Future work and improvements\")\n",
        "\n",
        "print(\"\\nProject completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
